{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/unlabeled/relationships/r.txt\n",
      "18M [Moore, Oklahoma]I just turned 18, I was a child in a guardianship and now they want to kick me out, where do I start?\n",
      "../data/unlabeled/almosthomeless/almosthomeless.txt\n",
      "18M [Moore, Oklahoma]I just turned 18, I was a child in a guardianship and now they want to kick me out, where do I start?\n"
     ]
    }
   ],
   "source": [
    "pathlist = Path('../../data/unlabeled').glob('**/*.txt')\n",
    "\n",
    "stcs = []\n",
    "\n",
    "for path in pathlist:\n",
    "     # because path is object not string\n",
    "    path_in_str = str(path)\n",
    "    print(path_in_str)\n",
    "    f = open(path_in_str, \"r\") \n",
    "    text = f.read() \n",
    "    f.close()\n",
    "    stcs = stcs + sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(stcs,columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '../../data/preprocessed/'\n",
    "df = pd.read_pickle(path+'unlabeled_stcs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]{2,}') # remove number and words that length = 1\n",
    "df['processed_text'] = df['text'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(path+\"processed_unlabled.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket as s\n",
    "\n",
    "path = '../../data/preprocessed/'\n",
    "df = pd.read_pickle(path+'processed_unlabled.pkl')\n",
    "train = pd.read_pickle(path + \"train.pkl\")\n",
    "test = pd.read_pickle(path + \"test.pkl\")\n",
    "\n",
    "train_token = train['processed_text']\n",
    "test_token = test['processed_text']\n",
    "unlabeled_token = df['processed_text']\n",
    "\n",
    "X = train_token.append(test_token)\n",
    "X = X.reset_index(drop=True)\n",
    "\n",
    "corpus = X.append(unlabeled_token)\n",
    "corpus = corpus.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(corpus,\n",
    "                size=300,\n",
    "                window=10,\n",
    "                min_count=2,\n",
    "                workers=10,\n",
    "                iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_path = '../../models/'\n",
    "word2vec.save(m_path+'Word2Vec2.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model):\n",
    "\n",
    "        self.word_model = word_model\n",
    "        self.word_idf_weight = None\n",
    "        self.vector_size = word_model.wv.vector_size\n",
    "\n",
    "    def fit(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        \"\"\"\n",
    "        Fit in a list of docs, which had been preprocessed and tokenized,\n",
    "        such as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
    "        Then build up a tfidf model to compute each word's idf as its weight.\n",
    "        Noted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
    "        :param: pre_processed_docs: list of docs, which are tokenized\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "        text_docs = []\n",
    "        for doc in docs:\n",
    "            text_docs.append(\" \".join(doc))\n",
    "\n",
    "        tfidf = TfidfVectorizer() # default 1-gram \n",
    "        tfidf.fit(text_docs)  # must be list of text string\n",
    "\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
    "        self.word_idf_weight = defaultdict(lambda: max_idf,\n",
    "                                           [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
    "        return tfidf\n",
    "\n",
    "\n",
    "    def transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        doc_word_vector = self.word_average_list(docs)\n",
    "        return doc_word_vector\n",
    "\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        \"\"\"\n",
    "        Compute average word vector for a single doc/sentence.\n",
    "        :param sent: list of sentence tokens\n",
    "        :return:\n",
    "            mean: float of averaging word vectors\n",
    "        \"\"\"\n",
    "\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model.wv.vocab:\n",
    "                mean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            # If a text is empty, return a vector of zeros.\n",
    "            logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def word_average_list(self, docs):\n",
    "        \"\"\"\n",
    "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
    "        :param docs: list of sentence in list of separated tokens\n",
    "        :return:\n",
    "            array of average word vector in shape (len(docs),)\n",
    "        \"\"\"\n",
    "        return np.vstack([self.word_average(sent) for sent in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = Word2Vec.load(path+'Word2Vec.bin')\n",
    "tfidf_vec_tr = TfidfEmbeddingVectorizer(word2vec)\n",
    "tf_transformer = tfidf_vec_tr.fit(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tf_transformer, open(m_path+\"unlabeled_tfidf_transformer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Unigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_path = '../../models/'\n",
    "tf_transformer = pickle.load(open(m_path+\"unlabeled_tfidf_transformer.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,3)) #unigram, bigram, trigram\n",
    "# X_new = vectorizer.fit_transform(X)\n",
    "X_new = tf_transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_new.toarray()[:len(train)]\n",
    "X_test = X_new.toarray()[len(train):]\n",
    "y_train = train['label']\n",
    "y_test = test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict = {'log reg': LogisticRegression(random_state=42), \n",
    "            'naive bayes': GaussianNB(), \n",
    "            'linear svc': LinearSVC(random_state=42),\n",
    "            'sgd classifier': SGDClassifier(random_state=42),\n",
    "            'ada boost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "            'gradient boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "            'CART': DecisionTreeClassifier(random_state=42),\n",
    "            'random forest': RandomForestClassifier(n_estimators=100, random_state=42)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of log reg: 0.7440559440559441\n",
      "Accuracy of naive bayes: 0.5384615384615384\n",
      "Accuracy of linear svc: 0.7328671328671329\n",
      "Accuracy of sgd classifier: 0.7328671328671329\n",
      "Accuracy of ada boost: 0.6867132867132867\n",
      "Accuracy of gradient boosting: 0.6853146853146853\n",
      "Accuracy of CART: 0.5720279720279721\n",
      "Accuracy of random forest: 0.7020979020979021\n"
     ]
    }
   ],
   "source": [
    "for name, clf in clf_dict.items():\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy of {}:'.format(name), accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost: 0.6839160839160839\n",
      "Precision 0.6838046272493573\n",
      "Recall 0.7208672086720868\n",
      "F1-Score 0.7018469656992085\n"
     ]
    }
   ],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)\n",
    "params = {\"objective\":'multi:softprob','colsample_bytree': 0.3, 'learning_rate': 0.05,\n",
    "                'max_depth': 5, 'alpha': 10 , 'num_class': 2, 'random_state': 42}\n",
    "steps = 500  # The number of training iterations\n",
    "model = xgb.train(params, D_train, steps)\n",
    "preds = model.predict(D_test)\n",
    "y_pred = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "name = 'XGBoost'\n",
    "print('Accuracy of {}:'.format(name), accuracy_score(y_pred, y_test))\n",
    "print('Precision', precision_score(y_test, y_pred))\n",
    "print('Recall', recall_score(y_test, y_pred))\n",
    "print('F1-Score', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_doc_vec = tfidf_vec_tr.transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "tfidf_doc_vec_scaled = scaler.fit_transform(tfidf_doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_doc_vec[:len(train)]\n",
    "y_train = train['label']\n",
    "\n",
    "X_test = tfidf_doc_vec[len(train):]\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_dict = {'log reg': LogisticRegression(random_state=42), \n",
    "            'naive bayes': GaussianNB(), \n",
    "            'linear svc': LinearSVC(random_state=42),\n",
    "            'sgd classifier': SGDClassifier(random_state=42),\n",
    "            'ada boost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "            'gradient boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "            'CART': DecisionTreeClassifier(random_state=42),\n",
    "            'random forest': RandomForestClassifier(n_estimators=100, random_state=42)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of log reg: 0.7272727272727273\n",
      "Accuracy of naive bayes: 0.6615384615384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of linear svc: 0.6993006993006993\n",
      "Accuracy of sgd classifier: 0.6783216783216783\n",
      "Accuracy of ada boost: 0.6909090909090909\n",
      "Accuracy of gradient boosting: 0.7146853146853147\n",
      "Accuracy of CART: 0.5902097902097903\n",
      "Accuracy of random forest: 0.7286713286713287\n"
     ]
    }
   ],
   "source": [
    "for name, clf in clf_dict.items():\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy of {}:'.format(name), accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic reg: 0.7286713286713287\n",
      "Precision 0.7139364303178484\n",
      "Recall 0.7913279132791328\n",
      "F1-Score 0.750642673521851\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model = clf.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "name = 'logistic reg'\n",
    "print('Accuracy of {}:'.format(name), accuracy_score(y_pred, y_test))\n",
    "print('Precision', precision_score(y_test, y_pred))\n",
    "print('Recall', recall_score(y_test, y_pred))\n",
    "print('F1-Score', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)\n",
    "params = {\"objective\":'multi:softprob','colsample_bytree': 0.3, 'learning_rate': 0.05,\n",
    "                'max_depth': 5, 'alpha': 10 , 'num_class': 2, 'random_state': 42}\n",
    "steps = 500  # The number of training iterations\n",
    "model = xgb.train(params, D_train, steps)\n",
    "preds = model.predict(D_test)\n",
    "y_pred = np.asarray([np.argmax(line) for line in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [50, 100, 200, 300, 400, 500]\n",
    "for step in steps:\n",
    "    model = xgb.train(params, D_train, step)\n",
    "    preds = model.predict(D_test)\n",
    "    y_pred = np.asarray([np.argmax(line) for line in preds])\n",
    "    accs.append(accuracy_score(y_pred, y_test))\n",
    "    precisions.append(precision_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1s.append(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost: 0.7314685314685314\n",
      "Precision 0.7153284671532847\n",
      "Recall 0.7967479674796748\n",
      "F1-Score 0.7538461538461539\n"
     ]
    }
   ],
   "source": [
    "name = 'XGBoost'\n",
    "print('Accuracy of {}:'.format(name), accuracy_score(y_pred, y_test))\n",
    "print('Precision', precision_score(y_test, y_pred))\n",
    "print('Recall', recall_score(y_test, y_pred))\n",
    "print('F1-Score', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
