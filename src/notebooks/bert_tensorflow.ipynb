{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8I_lj3-mHn4F"
   },
   "source": [
    "# Stress Analysis in Social Media\n",
    "\n",
    "Leverage the newly published and labelled reddit dataset for stress analysis to develop and improve supervised learning methods for identifying stress, both neural and traditional, and analyze the complexity and diversity of the data and characteristics of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "qWxI2vgR3eSk",
    "outputId": "16a30e9d-9072-4827-d15b-671121b18d74"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "h1so5vxb3_X0",
    "outputId": "3ec554e3-353c-44c5-fe9e-10d17d7ad58a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.3-dlenv_tfe\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5546638750167621165\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14622910942808340127\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11877495865386256200\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15848577434\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10551595464284002639\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "qjFaTE-x4CrR",
    "outputId": "76549d55-31f9-49ef-a9e3-0aa8b6c2effc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpYq96Ru4_VH"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0NSxAlZk5Hxq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '../../data/preprocessed/' \n",
    "# path = '/content/Insight_Stress_Analysis/data/'\n",
    "train = pd.read_pickle(path + 'train.pkl')\n",
    "test = pd.read_pickle(path + 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2.,   34.,  865.,    0.,   85.,    0.,  595.,  129.,    0.,\n",
       "        1128.]),\n",
       " array([0.42857143, 0.48571429, 0.54285714, 0.6       , 0.65714286,\n",
       "        0.71428571, 0.77142857, 0.82857143, 0.88571429, 0.94285714,\n",
       "        1.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 98.,   0.,   0.,   0.,   0., 153.,  18.,   1., 180., 265.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['confidence']>0.8]\n",
    "test = test[test['confidence']>0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNxgtCKM6Fgl"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTnStJIC6ehF"
   },
   "source": [
    "#Data Preprocessing\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jqgfBG86e4u"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36eGXwot7AEO"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "RSKhGNLs7AYN",
    "outputId": "0faf1eef-1d33-46a8-8437-d72d037329ff"
   },
   "outputs": [],
   "source": [
    "# Load a vocabulary file and lowercasing information directly from the BERT tf hub module\n",
    "\n",
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, \n",
    "                                    do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fW7OR578KPJ"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. \n",
    "def get_max_len(text):\n",
    "    max_len = 0\n",
    "    for i in range(len(train)):\n",
    "        if len(text.iloc[i]) > max_len:\n",
    "            max_len = len(text.iloc[i])\n",
    "    return max_len\n",
    "\n",
    "temp = train.text.str.split(' ')\n",
    "max_len = get_max_len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgg9hJ-X7hEZ"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = max_len\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, \n",
    "                                                                  label_list, \n",
    "                                                                  MAX_SEQ_LENGTH, \n",
    "                                                                  tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, \n",
    "                                                                 label_list, \n",
    "                                                                 MAX_SEQ_LENGTH, \n",
    "                                                                 tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpZpSLpx9WpG"
   },
   "source": [
    "## Classification Model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mnA6t0XE7xvu"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
    "    bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\"output_bias\", \n",
    "                                  [num_labels], \n",
    "                                  initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yZIKI0N9ltF"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-H6S8v39Aaf"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(is_predicting, \n",
    "                                                               input_ids, \n",
    "                                                               input_mask, \n",
    "                                                               segment_ids, \n",
    "                                                               label_ids, \n",
    "                                                               num_labels)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(loss, \n",
    "                                                          learning_rate, \n",
    "                                                          num_train_steps, \n",
    "                                                          num_warmup_steps, \n",
    "                                                          use_tpu=False)\n",
    "\n",
    "            # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(label_ids, predicted_labels)\n",
    "                auc = tf.metrics.auc(label_ids, predicted_labels)\n",
    "                recall = tf.metrics.recall(label_ids, predicted_labels)\n",
    "                precision = tf.metrics.precision(label_ids, predicted_labels) \n",
    "                true_pos = tf.metrics.true_positives(label_ids, predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(label_ids, predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(label_ids, predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(label_ids, predicted_labels)\n",
    "\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(is_predicting, \n",
    "                                                         input_ids, \n",
    "                                                         input_mask, \n",
    "                                                         segment_ids, \n",
    "                                                         label_ids, \n",
    "                                                         num_labels)\n",
    "\n",
    "            predictions = {'probabilities': log_probs, 'labels': predicted_labels}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5XcUO5P9nIg"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "# Warmup is a period of time where the learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TU-DIr9u9v7r"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_cfdR7R9x0H"
   },
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "OUTPUT_DIR = 'output'\n",
    "\n",
    "run_config = tf.estimator.RunConfig(model_dir=OUTPUT_DIR,\n",
    "                                    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "                                    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "wMkSR3PE9y_b",
    "outputId": "33934e6b-ad0a-4887-9d3b-e7ba70f5730d"
   },
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(num_labels=len(label_list), \n",
    "                            learning_rate=LEARNING_RATE,\n",
    "                            num_train_steps=num_train_steps,\n",
    "                            num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                   config=run_config,\n",
    "                                   params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnlGLfJ1923J"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVzrvH3A9z7L"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(features=train_features,\n",
    "                                                      seq_length=MAX_SEQ_LENGTH,\n",
    "                                                      is_training=True,\n",
    "                                                      drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nqME57896sx"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lvXuYujx94yf",
    "outputId": "1d0a7cf5-2658-433a-ee38-42c39dd44a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "Training took time  0:03:48.131239\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "\n",
    "# train the model \n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVZXb-7A96eq"
   },
   "outputs": [],
   "source": [
    "# check the test result\n",
    "test_input_fn = run_classifier.input_fn_builder(features=test_features,\n",
    "                                                seq_length=MAX_SEQ_LENGTH,\n",
    "                                                is_training=False,\n",
    "                                                drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "xZuQBuIx-IW8",
    "outputId": "97134bd2-a292-4b3b-c1f7-12973fc484c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.927437,\n",
       " 'eval_accuracy': 0.9295302,\n",
       " 'f1_score': 0.9373134,\n",
       " 'false_negatives': 9.0,\n",
       " 'false_positives': 12.0,\n",
       " 'loss': 0.32066914,\n",
       " 'precision': 0.92899406,\n",
       " 'recall': 0.94578314,\n",
       " 'true_negatives': 120.0,\n",
       " 'true_positives': 157.0,\n",
       " 'global_step': 471}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kaZDDj_6-VTe"
   },
   "outputs": [],
   "source": [
    "def predict(in_sentences):\n",
    "    labels = [\"non-stress\", \"stress\"]\n",
    "    labels_idx = [0, 1]\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, \n",
    "                                                                 labels_idx, \n",
    "                                                                 MAX_SEQ_LENGTH, \n",
    "                                                                 tokenizer)\n",
    "    \n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, \n",
    "                                                       seq_length=MAX_SEQ_LENGTH, \n",
    "                                                       is_training=False, \n",
    "                                                       drop_remainder=False)\n",
    "\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    return [{\"text\": sentence, \"confidence\": list(prediction['probabilities']), \"labels\": labels[prediction['labels']]}\n",
    "            for sentence, prediction in zip(in_sentences, predictions)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAZ0qOZj-XdK"
   },
   "outputs": [],
   "source": [
    "pred_sentences = ['I have nowhere I can stay. I\\'m even getting rid of my beloved cat so I can have options.',\n",
    "    'i wanna die i swear i havent been so helpless and scared in so long.',\n",
    "    'I feel so hopeless about everything.',\n",
    "    \"It's Friday! We wish you a nice start into the weekend!\",\n",
    "\"Deep breathing exercises are very relaxing. It can also relieve the symptoms of stress and anxiety.\",\n",
    "\"Do you like fruits? I like so much! Be Happy, Keep Smiling!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lm66GDGR-cdV",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"I have nowhere I can stay. I'm even getting rid of my beloved cat so I can have options.\",\n",
       "  'confidence': [-4.6344376, -0.009758978],\n",
       "  'labels': 'stress'},\n",
       " {'text': 'i wanna die i swear i havent been so helpless and scared in so long.',\n",
       "  'confidence': [-6.711318, -0.0012178156],\n",
       "  'labels': 'stress'},\n",
       " {'text': 'I feel so hopeless about everything.',\n",
       "  'confidence': [-6.72842, -0.0011970982],\n",
       "  'labels': 'stress'},\n",
       " {'text': \"It's Friday! We wish you a nice start into the weekend!\",\n",
       "  'confidence': [-0.0007175017, -7.2401576],\n",
       "  'labels': 'non-stress'},\n",
       " {'text': 'Deep breathing exercises are very relaxing. It can also relieve the symptoms of stress and anxiety.',\n",
       "  'confidence': [-0.0010567086, -6.853126],\n",
       "  'labels': 'non-stress'},\n",
       " {'text': 'Do you like fruits? I like so much! Be Happy, Keep Smiling!',\n",
       "  'confidence': [-0.0009835887, -6.9248033],\n",
       "  'labels': 'non-stress'}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(pred_sentences)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small = test[:10]\n",
    "pred_sentences = small['text']\n",
    "predictions = predict(pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTSD is life changing for the worse. Every day fight. Getting in a fist fight in high school( who did not get into a fight)  I'm not talking about bullying,  Playing video games,  having bad dreams does not cause PTSD. We have seen things, or done things or ,been part of things,  that most people will never understand and hopefully never experience. End of Rant \n",
      " has been classified as non-stress and should be  1\n"
     ]
    }
   ],
   "source": [
    "for input, prediction, label in zip (pred_sentences, predictions,small['label']):\n",
    "    if (prediction['labels'] == \"non-stress\" and label==1) or (prediction['labels'] == \"stress\" and label==0):\n",
    "        print(input, '\\n has been classified as', prediction['labels'], 'and should be ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Ckpts to PB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "trained_checkpoint_prefix = 'Learn2Relax/learn2relax/notebooks/output/model.ckpt-471'\n",
    "export_dir = './bert_output/'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with tf.compat.v1.Session(graph=graph) as sess:\n",
    "    # Restore from checkpoint\n",
    "    loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')\n",
    "    loader.restore(sess, trained_checkpoint_prefix)\n",
    "\n",
    "#     # Export checkpoint to SavedModel\n",
    "    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "    builder.add_meta_graph_and_variables(sess,\n",
    "                                         [tf.saved_model.TRAINING, tf.saved_model.SERVING],\n",
    "                                         strip_default_attrs=True)\n",
    "    builder.save()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT fine-tuned model for Tensorflow serving\n",
    "Reference: https://medium.com/delvify/bert-rest-inference-from-the-fine-tuned-model-499997b32851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    label_ids = tf.placeholder(tf.int32, [None], name='label_ids')\n",
    "    input_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_ids')\n",
    "    input_mask = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_mask')\n",
    "    segment_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='segment_ids')\n",
    "    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
    "        'label_ids': label_ids,\n",
    "        'input_ids': input_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'segment_ids': segment_ids,\n",
    "    })()\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'output/1592238437'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator._export_to_tpu = False\n",
    "estimator.export_savedmodel(OUTPUT_DIR, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 310)\n",
      "        name: input_ids_1:0\n",
      "    inputs['input_mask'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 310)\n",
      "        name: input_mask_1:0\n",
      "    inputs['label_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1)\n",
      "        name: label_ids_1:0\n",
      "    inputs['segment_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 310)\n",
      "        name: segment_ids_1:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['labels'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: unknown_rank\n",
      "        name: loss/Squeeze:0\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 2)\n",
      "        name: loss/LogSoftmax:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "# !saved_model_cli show --dir /home/gillianchiang/Insight_Stress_Analysis/framework/output/1581039388/ --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=/models/output/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Stress_Analysis_BERT2",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf-gpu.1-15.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
